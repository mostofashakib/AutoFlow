{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "commandDict = {\n",
    "  \"slack\": [\"message\",\"repli\",\"call\"],\n",
    "  \"twillo\": [\"call\",\"text\",\"hang up\"],\n",
    "    \"googl\": 'BI',\n",
    "    \"sheet\": ['creat'],\n",
    "    \"doc\": [\"creat\"]\n",
    "}\n",
    "verbAbr = ['VBZ','VB','VBG']\n",
    "nounAbr = ['NNS','NN']\n",
    "\n",
    "abbDict = {\n",
    "    \"googl\" : \"Google\"\n",
    "}\n",
    "approvedVerbList = [\"repli\"]\n",
    "\n",
    "bigramDict = {\n",
    "    \"googl\":[\"sheet\",\"doc\"]\n",
    "}\n",
    "\n",
    "testStr = \"he replies on Slack, then save response into Google Sheets and make document on Google Sheets and hosting it on Slack\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "subjectList = ['boss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntag: else if\\ncondition:\\ncondition_API:\\nAction:\\nAction_API:\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tag: else if\n",
    "condition:\n",
    "condition_API:\n",
    "Action:\n",
    "Action_API:\n",
    "'''\n",
    "\n",
    "# Work with action\n",
    "# Work with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still work in progress\n",
    "def checkBigram(word,nextWord):\n",
    "    # if we encounter Googl\n",
    "    #print('word: ', word)\n",
    "    #print('nextWord: ', nextWord)\n",
    "    resStr = abbDict[word] + ' ' + nextWord\n",
    "    return resStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mapCondAndAPIEntities(condList, condAPIList):\n",
    "    resDict = {}\n",
    "    if len(condAPIList) <= 0:\n",
    "        print('No APIs were detected')\n",
    "        return None\n",
    "    \n",
    "    elif len(condAPIList) == 1:\n",
    "        # Only one API in condition term\n",
    "        resDict[condAPIList[0]] = condList\n",
    "    else:\n",
    "        # Multiple APIs in condition term\n",
    "        print(condAPIList)\n",
    "        \n",
    "        # Corner cases\n",
    "        \n",
    "        for i,api in enumerate(condAPIList):\n",
    "            if i >= len(condList):\n",
    "                i = len(condList)-1\n",
    "            \n",
    "            resDict[api] = condList[i]\n",
    "            \n",
    "    \n",
    "    return resDict\n",
    "\n",
    "# This is for getting condition attributes\n",
    "def getConditionTag(conditionSentList):\n",
    "    condAPIList = []\n",
    "    condList = []\n",
    "\n",
    "    # condition API\n",
    "    # condition\n",
    "\n",
    "    # Each API should have their own list of commands\n",
    "\n",
    "    tokenized_sent = tokenizer.tokenize(conditionSentList)\n",
    "    #tokenized_sent = [stemmer.stem(w) for w in tokenized_sent]\n",
    "    partsOfSpeech = nltk.pos_tag(tokenized_sent)\n",
    "\n",
    "    continueCont = False\n",
    "    #for w in thenList[0].split(' '):\n",
    "    for i,w in enumerate(tokenized_sent):\n",
    "        if continueCont:\n",
    "            continue\n",
    "        word = stemmer.stem(w)\n",
    "        if word in commandDict and word not in condAPIList:\n",
    "            # If we recognize this as an API name\n",
    "            \n",
    "            if commandDict[word] == 'BI':\n",
    "                # This is a bigram\n",
    "                nextWord = tokenized_sent[i+1]\n",
    "                combWord = checkBigram(word,nextWord)\n",
    "                condAPIList.append(combWord)\n",
    "                continueCont = True\n",
    "            else:\n",
    "                condAPIList.append(word)\n",
    "\n",
    "        verb = partsOfSpeech[i][1]\n",
    "        if verb in verbAbr:\n",
    "            # We recognize this as an action\n",
    "            condList.append(word)\n",
    "            \n",
    "        \n",
    "        if verb in nounAbr and word in approvedVerbList:\n",
    "            # A verb could be false labeled as a noun\n",
    "            # So we perform a double check\n",
    "            condList.append(word)\n",
    "        \n",
    "\n",
    "        #print(partsOfSpeech[i])\n",
    "        #print(word)\n",
    "        #print('---------------')\n",
    "\n",
    "\n",
    "    #print(partsOfSpeech)\n",
    "    #print('--------------------------------------------')\n",
    "    '''\n",
    "    print('condition: ', condList)\n",
    "    print('condition_API: ', condAPIList)\n",
    "\n",
    "    print('\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "    print(testStr)\n",
    "    '''\n",
    "\n",
    "    # This is our result\n",
    "    result = mapCondAndAPIEntities(condList, condAPIList)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# This is for getting Action attributes\n",
    "\n",
    "def getActionTag(actionSentList):\n",
    "    \n",
    "    actionAPIList = []\n",
    "    actionList = []\n",
    "\n",
    "    # condition API\n",
    "    # condition\n",
    "\n",
    "    # Each API should have their own list of commands\n",
    "\n",
    "    tokenized_sent = tokenizer.tokenize(actionSentList)\n",
    "    #tokenized_sent = [stemmer.stem(w) for w in tokenized_sent]\n",
    "    partsOfSpeech = nltk.pos_tag(tokenized_sent)\n",
    "    #print(partsOfSpeech)\n",
    "\n",
    "    continueCont = False\n",
    "    #for w in thenList[0].split(' '):\n",
    "    for i,w in enumerate(tokenized_sent):\n",
    "        if continueCont:\n",
    "            continue\n",
    "        word = stemmer.stem(w)\n",
    "        if word in commandDict and word not in actionAPIList:\n",
    "            # If we recognize this as an API name\n",
    "            # If we recognize this as an API name\n",
    "            \n",
    "            if commandDict[word] == 'BI':\n",
    "                # This is a bigram\n",
    "                nextWord = tokenized_sent[i+1]\n",
    "                combWord = checkBigram(word,nextWord)\n",
    "                actionAPIList.append(combWord)\n",
    "                continueCont = True\n",
    "            \n",
    "            else:\n",
    "                actionAPIList.append(word)\n",
    "\n",
    "        verb = partsOfSpeech[i][1]\n",
    "        if verb in verbAbr:\n",
    "            # We recognize this as an action\n",
    "            actionList.append(word)\n",
    "        #print('verb: ', verb)\n",
    "        #print(nounAbr)\n",
    "        \n",
    "        if verb in nounAbr and word in approvedVerbList:\n",
    "            # A verb could be false labeled as a noun\n",
    "            # So we perform a double check\n",
    "            actionList.append(word)\n",
    "\n",
    "        #print(partsOfSpeech[i])\n",
    "        #print(word)\n",
    "        #print('---------------')\n",
    "\n",
    "\n",
    "    #print(partsOfSpeech)\n",
    "    #print('--------------------------------------------')\n",
    "    '''\n",
    "    print('action: ', actionList)\n",
    "    print('action_API: ', actionAPIList)\n",
    "\n",
    "    print('\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "    print(testStr)\n",
    "    '''\n",
    "\n",
    "    # This is our result\n",
    "    result = mapCondAndAPIEntities(actionList, actionAPIList)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# THIS WILL BE THE MAIN FUNCTION YOU WILL CALL\n",
    "def getCondAndActionTag(inputStr): \n",
    "\n",
    "    #print(testStr,'\\n\\n\\n\\n')\n",
    "\n",
    "    # THEN is a key word (for splitting if statements)\n",
    "    thenList = inputStr.split('then')\n",
    "\n",
    "    assert len(thenList) == 2\n",
    "\n",
    "    condTag = getConditionTag(thenList[0])\n",
    "    #print('Condition Tag: ', condTag)\n",
    "\n",
    "    actionList = []\n",
    "    for action in thenList[1].split('and'):\n",
    "        actionTag = getActionTag(action)\n",
    "        #print('Action Tag: ', actionTag)\n",
    "        actionList.append(actionTag)\n",
    "    return condTag,actionList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition Tag:  {'slack': ['repli']}\n",
      "Action:  {'Google Sheets': ['save']}\n",
      "Action:  {'Google Sheets': ['make']}\n",
      "Action:  {'slack': ['host']}\n"
     ]
    }
   ],
   "source": [
    "# Sample code to run the function\n",
    "\n",
    "condTag,actionList = getCondAndActionTag(testStr)\n",
    "\n",
    "print('Condition Tag: ', condTag)\n",
    "\n",
    "for action in actionList:\n",
    "    print('Action: ', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
